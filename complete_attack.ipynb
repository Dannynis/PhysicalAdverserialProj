{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e193bf91",
   "metadata": {},
   "source": [
    "## gather mutltiview samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce840fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capture_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('frame', cap.read()[1])\n",
    "# cv2.imwrite('test.png', cap.read()[1])\n",
    "# plt.imshow(cap.read()[1])\n",
    "# plt.show()\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_drawer()\n",
    "run_aruco_detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometric_calibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adv_optimization_utils as adv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5764a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv.optimize_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2378f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from classfier import *\n",
    "res_dir = './results/'\n",
    "# chose most recent result\n",
    "import os\n",
    "results = os.listdir(res_dir)\n",
    "results = [r for r in results if 'working_patch_' in r]\n",
    "results = sorted(results, key=lambda x: os.path.getmtime(os.path.join(res_dir, x)))\n",
    "load_path = res_dir + results[-1]\n",
    "print('loading from:', load_path)\n",
    "all_patchs_results = os.listdir(res_dir)\n",
    "all_patchs_results = [r for r in all_patchs_results if 'working_patchs_all_multiview' in r]\n",
    "all_patchs_results = sorted(all_patchs_results, key=lambda x: os.path.getmtime(os.path.join(res_dir, x)))\n",
    "load_path_all = res_dir + all_patchs_results[-1]\n",
    "print('loading all from:', load_path_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "patchs = torch.load(load_path_all)#torch.load('./results/working_patchs_all_multiview_2025-09-18_00_20.pth')\n",
    "tt = lambda x: torch.tensor(cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255.).permute(2,0,1).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "found = False\n",
    "good_patchs = []\n",
    "for idx, pp in enumerate(patchs[1]):\n",
    "    if 'jeep'  in patchs[0][idx] or 'half track' in patchs[0][idx]:\n",
    "        continue\n",
    "\n",
    "    plot_on_screen(pp.cpu().permute(1,2,0).numpy()*255)\n",
    "    for i in range(10):\n",
    "        r = cap.read()[1]\n",
    "        r = cv2.cvtColor(r, cv2.COLOR_BGR2RGB)\n",
    "        tr = tt(r)\n",
    "        res= resnet_predict(tr.cuda().unsqueeze(0))\n",
    "        # add text\n",
    "        cv2.putText(r, f'Pred: {res}', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "        cv2.imshow('frame', cv2.cvtColor(r, cv2.COLOR_BGR2RGB))\n",
    "        preds.append(res)\n",
    "        plt.imshow(r)\n",
    "        plt.show()\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        if 'jeep' not in res and 'half track' not in res:\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        good_patchs.append(pp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_on_screen(good_patchs[0].cpu().permute(1,2,0).numpy()*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pairs = list(zip(patchs[0], range(len(patchs[0]))))\n",
    "# print(pred_pairs)\n",
    "# for p in pred_pairs:\n",
    "#     if p[0] != 'jeep' and p[0] != 'half track':       \n",
    "#         print(p[0])\n",
    "#         patch = patchs[1][p[1]]\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# patch = torch.load('./results/working_patch_multiview_2025-09-17_07:23.pth')\n",
    "# tt = lambda x: torch.tensor(cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255.).permute(2,0,1).float()\n",
    "# patch = torch.load(load_path)\n",
    "# plot_on_screen(patch.cpu().permute(1,2,0).numpy()*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = []\n",
    "results = []\n",
    "for i in range(1000):\n",
    "    r = cap.read()[1]\n",
    "    tr = tt(r)\n",
    "    res= resnet_predict(tr.cuda().unsqueeze(0))\n",
    "    print(res)\n",
    "    # add text\n",
    "    cv2.putText(r, f'Pred: {res}', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow('frame', r)\n",
    "    results.append(res)\n",
    "\n",
    "    caps.append(r)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad575e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'captured_video_jeep2.avi'\n",
    "height, width, layers = caps[0].shape\n",
    "video = cv2.VideoWriter(video_name, 0, 30, (width,height))\n",
    "for captured in caps:\n",
    "    video.write(captured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cd6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from IPython.display import display, Image, clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "import kornia\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "\n",
    "tt = torchvision.transforms.ToTensor()\n",
    "\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import numpy as np\n",
    "\n",
    "from classfier import *\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"photometric_calibration.pkl\", \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "height = data['height']\n",
    "width = data['width']\n",
    "\n",
    "resizer = torchvision.transforms.Resize((height, width))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Define parameters for ArUco marker detection\n",
    "aruco_dict_type = cv2.aruco.DICT_6X6_250 # Change dictionary type if needed\n",
    "marker_length = 0.05  # Marker length in meters (adjust as needed)\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "\n",
    "marker_id = 40\n",
    "marker_size = 500  # Size in pixels\n",
    "marker_image = cv2.aruco.generateImageMarker(aruco_dict, marker_id, marker_size)\n",
    "\n",
    "\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "parameters = aruco.DetectorParameters()\n",
    "\n",
    "# Detect ArUco markers\n",
    "detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load stable diffusion model\n",
    "\n",
    "\n",
    "def decode_latents_grad(latents):\n",
    "    # latents = F.interpolate(latents, (64, 64), mode='bilinear', align_corners=False)\n",
    "    latents = 1 / 0.18215 * latents\n",
    "\n",
    "    imgs = vae.decode(latents).sample\n",
    "\n",
    "    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "def decode_latents(latents):\n",
    "    # latents = F.interpolate(latents, (64, 64), mode='bilinear', align_corners=False)\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device):\n",
    "            latents = 1 / 0.18215 * latents\n",
    "\n",
    "            with torch.no_grad():\n",
    "                imgs = vae.decode(latents).sample\n",
    "\n",
    "            imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "def encode_imgs(imgs):\n",
    "    # imgs: [B, 3, H, W]\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device):\n",
    "            imgs = 2 * imgs - 1\n",
    "\n",
    "            posterior = vae.encode(imgs).latent_dist\n",
    "            latents = posterior.sample() * 0.18215\n",
    "\n",
    "    return latents\n",
    "\n",
    "\n",
    "\n",
    "class framesDataset(Dataset):\n",
    "    def __init__(self, frames, Hs):\n",
    "        self.frames = frames\n",
    "        self.Hs = Hs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        H = self.Hs[idx]\n",
    "\n",
    "        # Convert to tensor and normalize\n",
    "        frame_tensor = tt(frame)\n",
    "\n",
    "        return frame_tensor, H.astype(np.float32)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def warp(decoded_latents,H_t):\n",
    "    dst_img_shape = valid_frames[0].shape[:2]\n",
    "    warped_imgs = []\n",
    "    for decoded_latent in decoded_latents:\n",
    "        img = decoded_latent.unsqueeze(0).float().repeat(H_t.shape[0], 1, 1, 1)\n",
    "        w=  kornia.geometry.transform.warp_perspective(img, H_t, dst_img_shape)\n",
    "        warped_imgs.append(w)\n",
    "    return torch.stack(warped_imgs, dim=0)#.squeeze(1)\n",
    "\n",
    "\n",
    "vae = None\n",
    "valid_frames = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1842c2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43d070fd5fd4da2b2e40f8d6f156f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using capture dir captures_frames_multiview_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_50544\\1026747868.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for path in tqdm.tqdm_notebook(valid_frame_paths):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481e091bcaa5446aabf52f41b8d3f56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 valid frames with ArUco markers and original classes.\n",
      "creating dataloader\n",
      "dataloader created\n",
      "starting optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:45<00:00,  2.13it/s]\n",
      " 19%|█▊        | 18/97 [00:08<00:35,  2.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# adv_loss = lbfgs_latent_opt.step(latent_closure_adp)\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m blend_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m    108\u001b[0m stronger_aug_steps_thresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    \n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    curr_without_sec = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    curr_without_sec = curr_without_sec.replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "\n",
    "    vae =  pipe.vae.to(device).eval()\n",
    "\n",
    "    latent = (torch.rand((30,4, 4, 4), device=device) - 0.5) * 2\n",
    "\n",
    "    # latent = torch.load(r\"C:\\git\\PhysicalAdverserialProj\\results\\working_latent_multiview_2025-09-15_20\").to(device)\n",
    "    latent = latent.clone().detach()\n",
    "    with torch.no_grad():\n",
    "        decoded_latents = resizer(decode_latents(latent))\n",
    "\n",
    "\n",
    "    l_size_h = decoded_latents.shape[-2]\n",
    "    l_size_w = decoded_latents.shape[-1]\n",
    "\n",
    "    orig_img_corners = np.array([[0,0],[l_size_w,0],[l_size_w,l_size_h],[0,l_size_h]], dtype=np.float32)\n",
    "\n",
    "\n",
    "    border_size = 0\n",
    "\n",
    "    ls = os.listdir('.')\n",
    "    captures = [f for f in ls if f.startswith('captures_frames_multiview_') ]\n",
    "    cap_dir = f'captures_frames_multiview_{len(captures)-1}'\n",
    "    print('using capture dir', cap_dir)\n",
    "\n",
    "    valid_frame_paths = glob.glob(f'{cap_dir}/*.png')\n",
    "\n",
    "    valid_frames = []\n",
    "\n",
    "    Hs = []\n",
    "\n",
    "    for path in tqdm.tqdm_notebook(valid_frame_paths):\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        corners, ids, _ = detector.detectMarkers(gray)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pr = resnet_predict_raw(tt(img).cuda().unsqueeze(0))\n",
    "\n",
    "        if ids is not None and pr.argmax(1) in orig_clases:\n",
    "\n",
    "            c = corners[0][0]\n",
    "            unbordred_corners = np.array([[c[0][0]-border_size, c[0][1]+border_size],\n",
    "                                [c[1][0]-border_size, c[1][1]-border_size],\n",
    "                                [c[2][0]+border_size, c[2][1]-border_size],\n",
    "                                [c[3][0]+border_size, c[3][1]+border_size]])\n",
    "\n",
    "            dst_pts = unbordred_corners\n",
    "            H, _ = cv2.findHomography(orig_img_corners, dst_pts, cv2.RANSAC)\n",
    "\n",
    "            Hs.append(H)\n",
    "            valid_frames.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    print(f\"Found {len(valid_frames)} valid frames with ArUco markers and original classes.\")\n",
    "\n",
    "\n",
    "\n",
    "    ds = framesDataset(valid_frames, Hs)\n",
    "\n",
    "\n",
    "    print('creating dataloader')\n",
    "    train,test = torch.utils.data.random_split(ds, [int(len(ds)*0.8), len(ds)-int(len(ds)*0.8)])\n",
    "    train_loader = DataLoader(train, batch_size=1, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test, batch_size=10, shuffle=False, num_workers=0)\n",
    "\n",
    "    print('dataloader created')\n",
    "        \n",
    "    latent.requires_grad = True\n",
    "\n",
    "\n",
    "    latent_opt = torch.optim.Adam([latent], lr=0.1)\n",
    "\n",
    "\n",
    "    latent.data = torch.load(\"./results/working_latent_multiview_2025-09-18_20_00.pth\").to(device)\n",
    "    \n",
    "\n",
    "\n",
    "    jitter = T.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1)\n",
    "    jitter_total_photo = T.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1)\n",
    "    jitter_with_hue = T.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.1)\n",
    "\n",
    "    augmentor = data['augmentor'].to(device).eval()\n",
    "    mapper = lambda x: jitter(augmentor(x))\n",
    "    # mapper = lambda x: jitter(x)\n",
    "\n",
    "    print('starting optimization')\n",
    "    i = 0 \n",
    "    # blend_ratio = 1\n",
    "    for epoch in range(50):\n",
    "        for frames, H_t in tqdm.tqdm(train_loader):\n",
    "\n",
    "            i += 1\n",
    "            # adv_loss = lbfgs_latent_opt.step(latent_closure_adp)\n",
    "\n",
    "            frames = frames.to('cuda')\n",
    "\n",
    "            blend_ratio = torch.rand(1).cuda() * 0.2 + 0.8\n",
    "            stronger_aug_steps_thresh = 400\n",
    "            \n",
    "            if i == stronger_aug_steps_thresh:\n",
    "                print('using stronger augmentations')\n",
    "\n",
    "            if i < stronger_aug_steps_thresh:\n",
    "                mapper = lambda x: jitter(x)\n",
    "\n",
    "            else:\n",
    "                mapper = lambda x: jitter(torch.stack([augmentor(xx).to(device) for xx in x]))\n",
    "\n",
    "            latent_opt.zero_grad()\n",
    "            adv_patch = resizer(decode_latents_grad(latent).float())\n",
    "\n",
    "            adv_patch_m = mapper(adv_patch)\n",
    "\n",
    "            # w_mask  =warp(adv_patch_m*0+1)\n",
    "            # w  =warp(adv_patch_m)\n",
    "            w_mask  =warp(adv_patch_m*0+1, H_t.cuda())\n",
    "            w  = warp(adv_patch_m, H_t.cuda())\n",
    "\n",
    "            sum_tensor = ((w_mask != 0) * -blend_ratio + 1) * frames + w *blend_ratio\n",
    "\n",
    "            sum_tensor = sum_tensor.view(sum_tensor.shape[0]*sum_tensor.shape[1], sum_tensor.shape[2], sum_tensor.shape[3], sum_tensor.shape[4])\n",
    "\n",
    "            sum_tensor = jitter_total_photo(sum_tensor)\n",
    "\n",
    "            adv_loss = adv_loss_calc2(sum_tensor).mean() #+ adv_patch.norm() / 10000\n",
    "            \n",
    "            adv_loss.backward()\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                latent_opt.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                unsuccessful_patches_idxs = torch.arange(0, sum_tensor.shape[0])[torch.tensor([x in orig_clases for x in resnet_predict_raw(sum_tensor).argmax(1)])]\n",
    "                successful_patches_idxs = torch.arange(0, sum_tensor.shape[0])[torch.tensor([x not in orig_clases for x in resnet_predict_raw(sum_tensor).argmax(1)])]\n",
    "                for idx in unsuccessful_patches_idxs:\n",
    "                    if successful_patches_idxs.shape[0] != 0:\n",
    "                        rand_succ_idx = successful_patches_idxs[torch.randint(0, successful_patches_idxs.shape[0], (1,)).item()]\n",
    "                        latent.data[idx] = latent.data[rand_succ_idx] + (torch.randn_like(latent.data[rand_succ_idx]) * 0.05)\n",
    "                    else:\n",
    "                        latent.data[idx] = (torch.rand((4, 4, 4), device=device) - 0.5) * 2\n",
    "\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                # print(f\"Iteration {i}, Loss: {latent_closure_adp().item():.4f}\")\n",
    "                print(f\"Iteration {i}, Loss: {adv_loss.item():.4f}. grad_norm: {latent.grad.norm().item():.4f}\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for frames, H_t in tqdm.tqdm(test_loader):\n",
    "\n",
    "                        adv_patch = resizer(decode_latents(latent).float())\n",
    "\n",
    "                        frames = frames.to('cuda')\n",
    "\n",
    "                        \n",
    "                        adv_patch_m = mapper(adv_patch)\n",
    "\n",
    "                        # w_mask  =warp(adv_patch_m*0+1)\n",
    "                        # w  =warp(adv_patch_m)\n",
    "                        w_mask  =warp(adv_patch_m*0+1, H_t.cuda())\n",
    "                        w  = warp(adv_patch_m, H_t.cuda())\n",
    "\n",
    "                        sum_tensor = ((w_mask != 0) * -blend_ratio + 1) * frames + w *blend_ratio\n",
    "\n",
    "                        sum_tensor = sum_tensor.view(sum_tensor.shape[0]*sum_tensor.shape[1], sum_tensor.shape[2], sum_tensor.shape[3], sum_tensor.shape[4])\n",
    "\n",
    "                        sum_tensor = jitter_total_photo(sum_tensor)\n",
    "\n",
    "                        adv_loss = adv_loss_calc2(sum_tensor)\n",
    "\n",
    "                        sum_tensor = sum_tensor.cpu()\n",
    "                        argmin = torch.argmin(adv_loss).cpu().item()\n",
    "\n",
    "                        plt.imshow(sum_tensor[argmin].permute(1,2,0).numpy())\n",
    "                        plt.show()\n",
    "                        print(resnet_predict(sum_tensor[argmin].unsqueeze(0).cuda()))\n",
    "\n",
    "                        test_pred = resnet_predict(sum_tensor.cuda())\n",
    "                        if type(test_pred) == list:\n",
    "                            pred = resnet_predict(sum_tensor.cuda())\n",
    "                            print(np.unique(pred,return_counts=True))\n",
    "                        # print(lx)\n",
    "\n",
    "                        out_path =  rf'./results/working_latent_multiview_{curr_without_sec}.pth'\n",
    "                        torch.save(latent.cpu().detach(), out_path)\n",
    "                        print('saved latent to', out_path)\n",
    "                        out_path =  rf'./results/working_patch_multiview_{curr_without_sec}.pth'\n",
    "                        torch.save(adv_patch[argmin// sum_tensor.shape[0]].cpu().detach(), out_path)\n",
    "                        print('saved patch to', out_path)\n",
    "                        out_path =  rf'./results/working_patchs_all_multiview_{curr_without_sec}.pth'\n",
    "                        torch.save((pred,adv_patch.cpu().detach()), out_path)\n",
    "                        print('saved patch to', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd5657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6226, -1.8115,  1.7012,  0.0859],\n",
       "          [-0.1678, -2.8223, -4.7617,  1.4131],\n",
       "          [-0.9912, -3.6777, -0.4551, -1.4316],\n",
       "          [ 0.0331, -4.0977, -0.1975, -2.1953]],\n",
       "\n",
       "         [[-4.1055, -4.5781, -3.1504, -1.6455],\n",
       "          [-0.5840, -0.8682,  2.4238, -1.5693],\n",
       "          [-1.7598, -5.1445, -3.3164, -0.4890],\n",
       "          [ 0.2874, -0.6660, -2.7363, -0.6973]],\n",
       "\n",
       "         [[-2.8086, -0.1630,  2.9395, -2.4727],\n",
       "          [-2.1875,  0.4172, -1.1025, -1.7402],\n",
       "          [-1.5420,  2.6367, -0.0565, -2.4863],\n",
       "          [-0.6499,  0.1631, -0.1801,  1.1807]],\n",
       "\n",
       "         [[ 0.8496,  0.4580,  2.5488,  3.1641],\n",
       "          [ 0.8945,  0.7461,  1.5439,  1.8877],\n",
       "          [-0.2379,  1.1299, -1.4082,  1.8057],\n",
       "          [ 2.2266, -1.5205,  2.7227,  1.3740]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7012,  0.6582, -0.0848,  1.0977],\n",
       "          [-0.0061,  2.2129,  2.0469, -0.6880],\n",
       "          [-2.2402, -1.1943, -1.1074, -0.6025],\n",
       "          [-3.3633, -1.5498, -2.6660, -1.1260]],\n",
       "\n",
       "         [[-1.9707, -0.5518,  0.7979, -3.4902],\n",
       "          [ 2.3711, -2.2227, -1.7119,  1.0615],\n",
       "          [-1.3818,  3.4473,  2.8594, -0.6938],\n",
       "          [ 1.4492,  2.1406, -0.5859, -1.1719]],\n",
       "\n",
       "         [[-3.5801,  0.2463,  1.2393, -4.3594],\n",
       "          [ 1.7256,  1.8379, -1.7822,  3.7812],\n",
       "          [ 2.9531,  2.3184,  0.2162,  3.9922],\n",
       "          [ 2.7344,  1.8652, -0.2150,  0.4250]],\n",
       "\n",
       "         [[ 0.4661,  2.9707,  0.3296, -0.6440],\n",
       "          [ 3.3848, -0.1021,  2.1621, -0.7471],\n",
       "          [-1.7275,  2.2129, -1.2266,  1.1436],\n",
       "          [ 0.0988,  1.8291,  0.3933,  1.4902]]],\n",
       "\n",
       "\n",
       "        [[[-0.2520,  1.0029, -0.2058,  2.1406],\n",
       "          [-1.0381, -1.3262, -2.4629,  2.4160],\n",
       "          [-3.1582,  0.7627, -0.9663,  0.7334],\n",
       "          [-1.0283, -1.5918, -1.9570, -1.0977]],\n",
       "\n",
       "         [[-1.5967, -1.8281, -0.6743,  0.2003],\n",
       "          [-1.0742,  1.6465, -0.1975, -3.2910],\n",
       "          [-1.0254, -0.6147, -1.1396,  1.4756],\n",
       "          [ 1.5107,  1.8838,  1.2666, -1.3779]],\n",
       "\n",
       "         [[-2.3359,  3.2852,  1.7842,  4.8945],\n",
       "          [-0.8613, -0.5962, -1.0850, -1.9463],\n",
       "          [ 1.9004,  1.7842,  3.5176, -0.5166],\n",
       "          [-0.3855,  3.0469,  4.1992,  1.9395]],\n",
       "\n",
       "         [[ 0.2671,  1.8096,  0.7798,  2.7031],\n",
       "          [ 3.5195,  0.3167,  1.1729,  0.8364],\n",
       "          [-0.5479,  0.4763, -1.5596,  2.7266],\n",
       "          [-0.1882, -1.8438, -0.0118,  1.7842]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0901, -1.6348,  1.8154, -2.8809],\n",
       "          [-0.7061, -1.5225, -0.9570,  1.4277],\n",
       "          [-1.9668, -0.6030,  3.2871, -1.2432],\n",
       "          [ 1.3730,  0.8008,  1.5098, -1.7979]],\n",
       "\n",
       "         [[-2.2070, -1.0752,  1.0215, -0.6133],\n",
       "          [-0.8130,  0.1519,  0.7324, -0.7124],\n",
       "          [ 0.8203,  0.4412, -1.4385,  0.7358],\n",
       "          [-4.2812,  1.2422,  0.5781, -1.0195]],\n",
       "\n",
       "         [[-3.8789,  2.5859, -1.9658,  1.4053],\n",
       "          [ 0.0284, -2.4707,  3.6367, -3.0156],\n",
       "          [ 0.3755, -1.4912,  0.1173,  3.2227],\n",
       "          [-1.2607,  3.8711,  3.0723,  3.4297]],\n",
       "\n",
       "         [[-2.9160,  3.9648,  0.5474, -1.2520],\n",
       "          [ 2.9688, -3.9180,  2.0488,  1.4844],\n",
       "          [-1.7373,  1.2354, -0.6167, -0.4561],\n",
       "          [ 0.7080, -1.0693,  0.9937,  0.7109]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6650,  0.7061,  0.9097,  0.9565],\n",
       "          [ 0.2732,  0.2935, -2.2344, -3.1562],\n",
       "          [-1.3926, -1.9834, -1.7373, -2.1504],\n",
       "          [-1.7305, -1.5283,  0.0123, -0.0926]],\n",
       "\n",
       "         [[-1.7451, -2.4375, -0.3232, -0.2927],\n",
       "          [-2.8027, -3.3223,  1.5947, -2.2871],\n",
       "          [-1.7686, -1.8184,  1.7178, -0.1929],\n",
       "          [ 2.6543,  0.5947, -2.0371,  3.6797]],\n",
       "\n",
       "         [[-2.1309, -1.1309,  1.3613,  0.4771],\n",
       "          [ 0.4590,  0.5210, -3.0000, -0.1666],\n",
       "          [ 0.7656, -1.5469,  0.2893, -1.3438],\n",
       "          [ 0.4409, -1.9922, -0.2913,  2.4199]],\n",
       "\n",
       "         [[ 0.6235, -0.3157,  0.6470,  1.3955],\n",
       "          [ 1.1582,  2.9473, -1.0850,  0.3914],\n",
       "          [ 0.7466, -3.3691,  1.7559,  3.5000],\n",
       "          [ 1.1445,  2.6309, -0.7686,  2.4336]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4958,  0.3831,  1.2949, -1.2705],\n",
       "          [-1.0820,  0.3662, -0.8374,  0.4365],\n",
       "          [-0.0170,  0.2698,  0.2510,  0.9082],\n",
       "          [-0.7378, -2.2070, -2.5430,  0.1949]],\n",
       "\n",
       "         [[-3.1211, -4.4023,  1.5508, -0.7031],\n",
       "          [-3.6348,  3.3633, -0.6167,  0.8716],\n",
       "          [ 0.8203, -0.5210, -0.2607, -0.3735],\n",
       "          [ 1.4141, -3.2012,  1.3037,  0.1256]],\n",
       "\n",
       "         [[-3.3984,  2.7422,  2.4785, -0.3704],\n",
       "          [-3.5117, -0.4907,  0.9463, -1.4736],\n",
       "          [-1.5547,  3.8574,  0.3523,  4.7656],\n",
       "          [-2.9902, -1.5996,  0.0500,  3.7051]],\n",
       "\n",
       "         [[ 1.0273,  0.1753,  0.3076,  0.6504],\n",
       "          [-1.3564,  2.7090,  1.9033,  3.4902],\n",
       "          [ 0.1652, -1.3330, -0.2440, -2.0781],\n",
       "          [ 1.7725, -0.4131,  1.0146,  0.7197]]]], device='cuda:0',\n",
       "       dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52477700",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(weights.meta[\"categories\"]):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e534e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
